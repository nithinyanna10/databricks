{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Pipeline Test\n",
        "\n",
        "This notebook demonstrates the complete data pipeline from ingestion to validation using sample data.\n",
        "\n",
        "## Pipeline Overview\n",
        "1. **Data Ingestion** (Bronze Layer)\n",
        "2. **Data Transformation** (Silver Layer) \n",
        "3. **Data Aggregation** (Gold Layer)\n",
        "4. **Data Quality Validation**\n",
        "\n",
        "## Sample Data\n",
        "We'll use a small sample dataset to test the entire pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Ingestion (Bronze Layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample data (Bronze Layer)\n",
        "from pyspark.sql.functions import current_timestamp, lit\n",
        "\n",
        "# Try to load from sample data first, fallback to databricks-datasets\n",
        "try:\n",
        "    df = spark.read.csv(\"/FileStore/sample_airlines.csv\", header=True, inferSchema=True)\n",
        "    print(\"âœ… Loaded sample data from FileStore\")\n",
        "except:\n",
        "    try:\n",
        "        df = spark.read.csv(\"/databricks-datasets/airlines\", header=True, inferSchema=True)\n",
        "        print(\"âœ… Loaded data from databricks-datasets\")\n",
        "    except:\n",
        "        # Create sample data if neither works\n",
        "        sample_data = [\n",
        "            (2001, 1, 1, 848, 8, 923, 3, \"AA\", 1, \"N319AA\", \"JFK\", \"LAX\", 339, 2475, 14, 8),\n",
        "            (2001, 1, 1, 850, 10, 1006, 6, \"AA\", 2, \"N319AA\", \"JFK\", \"LAX\", 336, 2475, 14, 10),\n",
        "            (2001, 1, 1, 923, 23, 1004, 4, \"AA\", 3, \"N319AA\", \"JFK\", \"LAX\", 321, 2475, 15, 23),\n",
        "            (2001, 1, 1, 1007, 7, 1130, 0, \"AA\", 4, \"N319AA\", \"JFK\", \"LAX\", 323, 2475, 16, 7),\n",
        "            (2001, 1, 1, 1249, 9, 1518, 8, \"AA\", 5, \"N319AA\", \"JFK\", \"LAX\", 329, 2475, 20, 49)\n",
        "        ]\n",
        "        columns = [\"year\", \"month\", \"day\", \"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \n",
        "                  \"carrier\", \"flight\", \"tailnum\", \"origin\", \"dest\", \"air_time\", \"distance\", \"hour\", \"minute\"]\n",
        "        df = spark.createDataFrame(sample_data, columns)\n",
        "        print(\"âœ… Created sample data\")\n",
        "\n",
        "# Add metadata columns for Bronze layer\n",
        "bronze_df = df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
        "bronze_df = bronze_df.withColumn(\"data_source\", lit(\"sample_data\"))\n",
        "bronze_df = bronze_df.withColumn(\"layer\", lit(\"bronze\"))\n",
        "\n",
        "print(f\"Bronze Layer - Raw Data:\")\n",
        "print(f\"Records: {bronze_df.count()}\")\n",
        "print(f\"Columns: {len(bronze_df.columns)}\")\n",
        "bronze_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Bronze layer\n",
        "bronze_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/airlines_bronze\")\n",
        "print(\"âœ… Bronze layer saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Transformation (Silver Layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Bronze data and clean it (Silver Layer)\n",
        "from pyspark.sql.functions import col, when, isnan, isnull, trim, upper\n",
        "\n",
        "bronze_df = spark.read.format(\"delta\").load(\"/delta/airlines_bronze\")\n",
        "\n",
        "print(\"Data Quality Checks:\")\n",
        "print(f\"Total records: {bronze_df.count()}\")\n",
        "print(f\"Records with null carrier: {bronze_df.filter(bronze_df.carrier.isNull()).count()}\")\n",
        "print(f\"Records with null flight: {bronze_df.filter(bronze_df.flight.isNull()).count()}\")\n",
        "\n",
        "# Clean and validate data\n",
        "silver_df = (bronze_df.filter(\n",
        "    col(\"carrier\").isNotNull() & \n",
        "    col(\"flight\").isNotNull() &\n",
        "    col(\"origin\").isNotNull() &\n",
        "    col(\"dest\").isNotNull()\n",
        ")\n",
        ".withColumn(\"carrier\", trim(upper(col(\"carrier\"))))\n",
        ".withColumn(\"origin\", trim(upper(col(\"origin\"))))\n",
        ".withColumn(\"dest\", trim(upper(col(\"dest\"))))\n",
        ".withColumn(\"layer\", lit(\"silver\"))\n",
        ".withColumn(\"processing_timestamp\", current_timestamp()))\n",
        "\n",
        "print(f\"\\nSilver Layer - Cleaned Data:\")\n",
        "print(f\"Records after cleaning: {silver_df.count()}\")\n",
        "silver_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Silver layer\n",
        "silver_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/airlines_silver\")\n",
        "print(\"âœ… Silver layer saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Aggregation (Gold Layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Silver data and create business metrics (Gold Layer)\n",
        "from pyspark.sql.functions import count, avg, sum, max, min\n",
        "\n",
        "silver_df = spark.read.format(\"delta\").load(\"/delta/airlines_silver\")\n",
        "\n",
        "# Create carrier performance metrics\n",
        "carrier_metrics = (silver_df.groupBy(\"carrier\").agg(\n",
        "    count(\"*\").alias(\"total_flights\"),\n",
        "    avg(\"dep_delay\").alias(\"avg_departure_delay\"),\n",
        "    avg(\"arr_delay\").alias(\"avg_arrival_delay\"),\n",
        "    avg(\"distance\").alias(\"avg_distance\")\n",
        ")\n",
        ".withColumn(\"layer\", lit(\"gold\"))\n",
        ".withColumn(\"aggregation_timestamp\", current_timestamp()))\n",
        "\n",
        "print(\"Gold Layer - Carrier Metrics:\")\n",
        "carrier_metrics.show()\n",
        "\n",
        "# Save Gold layer\n",
        "carrier_metrics.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/airlines_gold\")\n",
        "print(\"âœ… Gold layer saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Data Quality Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Validation\n",
        "from pyspark.sql.functions import col, count, isnan, isnull\n",
        "\n",
        "# Load Gold data for validation\n",
        "gold_df = spark.read.format(\"delta\").load(\"/delta/airlines_gold\")\n",
        "\n",
        "print(\"=== DATA QUALITY REPORT ===\")\n",
        "print(f\"Table: /delta/airlines_gold\")\n",
        "print(f\"Total records: {gold_df.count()}\")\n",
        "print(f\"Columns: {len(gold_df.columns)}\")\n",
        "\n",
        "# Check for null values\n",
        "print(\"\\n1. NULL VALUE ANALYSIS:\")\n",
        "for column in gold_df.columns:\n",
        "    null_count = gold_df.filter(col(column).isNull()).count()\n",
        "    total_count = gold_df.count()\n",
        "    null_percentage = (null_count / total_count) * 100 if total_count > 0 else 0\n",
        "    print(f\"  {column}: {null_count} nulls ({null_percentage:.2f}%)\")\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\n2. DUPLICATE ANALYSIS:\")\n",
        "duplicate_count = gold_df.count() - gold_df.dropDuplicates().count()\n",
        "print(f\"  Duplicate records: {duplicate_count}\")\n",
        "\n",
        "# Data freshness check\n",
        "print(\"\\n3. DATA FRESHNESS:\")\n",
        "if \"aggregation_timestamp\" in gold_df.columns:\n",
        "    latest_timestamp = gold_df.select(\"aggregation_timestamp\").orderBy(col(\"aggregation_timestamp\").desc()).first()[0]\n",
        "    print(f\"  Latest aggregation time: {latest_timestamp}\")\n",
        "else:\n",
        "    print(\"  No timestamp column found\")\n",
        "\n",
        "# Show final results\n",
        "print(\"\\n4. FINAL DATA:\")\n",
        "gold_df.show()\n",
        "\n",
        "print(\"\\nâœ… Pipeline completed successfully!\")\n",
        "print(\"ðŸ“Š Summary:\")\n",
        "print(f\"  - Bronze records: {spark.read.format('delta').load('/delta/airlines_bronze').count()}\")\n",
        "print(f\"  - Silver records: {spark.read.format('delta').load('/delta/airlines_silver').count()}\")\n",
        "print(f\"  - Gold records: {gold_df.count()}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
